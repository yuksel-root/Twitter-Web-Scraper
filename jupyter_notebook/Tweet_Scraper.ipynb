{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "import csv\n",
    "import pandas as pd\n",
    "from userinfo import uid,passw\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime\n",
    "import jellyfish\n",
    "import pyodbc\n",
    "\n",
    "\n",
    "#Defined Global Variables\n",
    "set_tweet  = set()   ;  same_tweet  = list()  ;  diff_tweet = list()\n",
    "dict_like  = dict()  ;  dict_reply  = dict()  ;  dict_rt    = dict()   \n",
    "dict_like2 = dict()  ;  dict_reply2 = dict()  ;  dict_rt2   = dict() \n",
    "\n",
    "#Tweet letter filter    \n",
    "def defined_letters_filter(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\" \",\n",
    "               'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    for i in tweet:\n",
    "        if(i not in letters):\n",
    "            tweet = tweet.replace(i, '')\n",
    "    return tweet\n",
    "\n",
    "#Erase the words in space\n",
    "def space_cleaner(tweet):\n",
    "    space = [\" \"]\n",
    "    for i in tweet:\n",
    "        if(i  in space):\n",
    "            tweet = tweet.replace(i,'')\n",
    "            return tweet\n",
    "    return tweet\n",
    "\n",
    "#Only ascii characters filter\n",
    "def ascii_filter(tweet):\n",
    "    letters = list()\n",
    "    letters.append('£') ; letters.append('é') ; letters.append('½') ; letters.append('¾')\n",
    "    for i in range(0,128):\n",
    "        letters.append(chr(i))\n",
    "        \n",
    "    for i in tweet:\n",
    "        if(i not in letters):\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "\n",
    "    \n",
    "#Link cleaning\n",
    "def link_clean(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = space_cleaner(tweet)\n",
    "    count = 0\n",
    "    count = tweet.count(\"http\")\n",
    "    \n",
    "    if(count != 0):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "#Defined tag filter\n",
    "def defined_tag_filter(tweet):\n",
    "    count = 0 ; count1 = 0 ; counts = 0\n",
    "    tweet = tweet.lower()\n",
    "    count1 = tweet.count(\"#\") ;\n",
    "    count += tweet.count(\"#trump\")\n",
    "    count += tweet.count(\"#usa\")\n",
    "    count += tweet.count(\"#fake\")\n",
    "    count += tweet.count(\"#country\")\n",
    "    count += tweet.count(\"#money\")\n",
    "    count += tweet.count(\"#donald\")\n",
    "    count += tweet.count(\"#joe\")\n",
    "    count += tweet.count(\"#pence\")\n",
    "    count += tweet.count(\"#white\")\n",
    "    count += tweet.count(\"#house\")\n",
    "    count += tweet.count(\"#win\")\n",
    "    count += tweet.count(\"#vote\")\n",
    "    count += tweet.count(\"#support\")\n",
    "    count += tweet.count(\"#president\")\n",
    "    count += tweet.count(\"#vpdebate\")\n",
    "    count += tweet.count(\"#debate\")\n",
    "    count += tweet.count(\"#potus\")\n",
    "    count += tweet.count(\"#november\")\n",
    "    count += tweet.count(\"#america\")\n",
    "    count += tweet.count(\"#election\")\n",
    "    counts = count1 - count  \n",
    "   \n",
    "    if(counts != 0):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "#Spam Control Function\n",
    "def similarity_control(tweet2):\n",
    "    tweet2 = defined_letters_filter(tweet2)\n",
    "    for i in set_tweet:\n",
    "        i = defined_letters_filter(i)\n",
    "        similarity  = jellyfish.levenshtein_distance(tweet2,i)\n",
    "        if(float(similarity / len(tweet2)) <= 0.4):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "#Fetch Data Functions\n",
    "def fetch_data(scroll_count):\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    #search hashtag since:2020-08-24 until:2020-10-10\n",
    "    search_hashtag_link = \"https://twitter.com/search?q=%23trump%20since%3A2020-08-24%20until%3A2020-10-10&src=typed_query&f=live\"\n",
    "    driver.get(search_hashtag_link)\n",
    "    time.sleep(20)\n",
    "    \n",
    "    \n",
    "#Fetch Start\n",
    "    tw_counts = 0 ; current_page = 0  \n",
    "    while (current_page < scroll_count):\n",
    "        time.sleep(5)\n",
    "        start_time2 = datetime.now(); \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source,\"html.parser\")\n",
    "        tweets = soup.find_all(\"div\",attrs={\"data-testid\":\"tweet\"})\n",
    "          \n",
    "        #Parse Processing...\n",
    "        for i in tweets:\n",
    "            try:\n",
    "                tweet  = i.find(\"div\",attrs={\"css-901oao r-hkyrab r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\"}).text\n",
    "                like = i.find(\"div\",attrs={\"data-testid\":\"like\"}).text\n",
    "                reply  = i.find(\"div\",attrs={\"data-testid\":\"reply\"}).text\n",
    "                rt     = i.find(\"div\",attrs={\"data-testid\":\"retweet\"}).text\n",
    "                \n",
    "                if ((reply == \"\")):\n",
    "                    reply = 0\n",
    "                if((like == \"\")):\n",
    "                    like = 0\n",
    "                if((rt == \"\")):\n",
    "                    rt = 0\n",
    "                    \n",
    "            #Data filter functions\n",
    "                if(ascii_filter(tweet) == True):\n",
    "                    if((link_clean(tweet)) == True):\n",
    "                        if((similarity_control(tweet) == True)):\n",
    "                            if(defined_tag_filter(tweet) == True):\n",
    "                                set_tweet.add(tweet)\n",
    "                                dict_like[tweet] = like\n",
    "                                dict_reply[tweet] = reply\n",
    "                                dict_rt[tweet] = rt\n",
    "                            else:\n",
    "                                same_tweet.append(tweet)\n",
    "                                dict_like2[tweet] = like\n",
    "                                dict_reply2[tweet] = reply\n",
    "                                dict_rt2[tweet] = rt\n",
    "                        else:\n",
    "                            same_tweet.append(tweet)\n",
    "                            dict_like2[tweet] = like\n",
    "                            dict_reply2[tweet] = reply\n",
    "                            dict_rt2[tweet] = rt\n",
    "               \n",
    "                    \n",
    "            except Exception as ex:\n",
    "                print(\"try error\")\n",
    "                print(ex)\n",
    "        \n",
    "        \n",
    "        #Total tweet Counting Process..\n",
    "        tw_count = len(tweets)\n",
    "        tw_counts += tw_count\n",
    "        current_page += 1\n",
    "        \n",
    "        \n",
    "        #Step Ending Print Process\n",
    "        end_time2 = datetime.now()\n",
    "        print(current_page,\".scroll\",tw_counts,\".tweet\")\n",
    "        print('Duration: {}'.format(end_time2 - start_time))       \n",
    "\n",
    "        \n",
    "        #Scroll Down Process\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            delay_start = datetime.now() ; print(delay_start)\n",
    "            print(\"can't more slip\") ; print(new_height,\" == \",last_height) ; print(\"delay 10sn\")\n",
    "            time.sleep(5)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(5)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if(new_height == last_height):\n",
    "                break\n",
    "            else:\n",
    "                delay_end = datetime.now() ; print(\"continue process... \",delay_end)\n",
    "                last_height = new_height\n",
    "                continue\n",
    "        last_height = new_height\n",
    "       \n",
    "        \n",
    "        #Delay Process..\n",
    "        if((current_page % 9000) == 0):\n",
    "            print(\"waiting ...10dk\")\n",
    "            start3_time = datetime.now()\n",
    "            print(\"Duration date\",start3_time)\n",
    "            time.sleep(600)\n",
    "    \n",
    "    \n",
    "    #Print Processing\n",
    "    print(\"total_tweet\",tw_counts)\n",
    "    print(\" len diff_tweet=\",len(set_tweet))\n",
    "    print(\" len same_tweet=\",len(same_tweet))\n",
    "    \n",
    "        \n",
    "#Db Write Process\n",
    "    diff_tweet = list(set_tweet) ; x = 0 ; y = 0\n",
    "    con = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                  'SERVER=localhost;'\n",
    "                  'DATABASE=abdElection2020;'\n",
    "                  'UID=' + uid + \";\"\n",
    "                  'PWD=' + passw + \";\")\n",
    "   \n",
    "       \n",
    "    for i in diff_tweet:\n",
    "        c = con.cursor()\n",
    "        query = 'INSERT INTO trump2020 VALUES(?,?,?,?,?)' # insert to database table original tweets\n",
    "        data = (x,diff_tweet[x],dict_like[i],dict_reply[i],dict_rt[i])\n",
    "        c.execute(query,data)\n",
    "        con.commit()\n",
    "        x += 1\n",
    "\n",
    "    for j in same_tweet:\n",
    "        c2 = con.cursor()\n",
    "        query2 = 'INSERT INTO trump2020same VALUES(?,?,?,?,?)' # insert to database table spam tweets\n",
    "        data2 = (y,same_tweet[y],dict_like2[j],dict_reply2[j],dict_rt2[j])\n",
    "        c2.execute(query2,data2)\n",
    "        con.commit()\n",
    "        y += 1\n",
    "        \n",
    "    print(\"succesfully db writing finish...\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Process\n",
    "start_time = datetime.now()\n",
    "print(\"starting=\",start_time)\n",
    "\n",
    "scroll_count = 10\n",
    "fetch_data(scroll_count)\n",
    "print(\"data fetch succesfully finish...\")\n",
    "    \n",
    "end_time = datetime.now()\n",
    "print('total_Duration: {}'.format(end_time - start_time))\n",
    "print(\"start_time:\",start_time,\" end_time:\",end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
